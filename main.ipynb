{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Package Installation","metadata":{}},{"cell_type":"markdown","source":"## Import necessary packages","metadata":{}},{"cell_type":"code","source":"!pip install ftfy\n!pip install timm","metadata":{"execution":{"iopub.status.busy":"2023-03-31T04:41:28.285629Z","iopub.execute_input":"2023-03-31T04:41:28.286197Z","iopub.status.idle":"2023-03-31T04:41:51.897311Z","shell.execute_reply.started":"2023-03-31T04:41:28.286091Z","shell.execute_reply":"2023-03-31T04:41:51.896155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os # file management\nimport torch\nfrom torch import nn, Tensor # neural network\nfrom torch.nn import functional as F\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport numpy as np \nfrom functools import partial\nimport timm\n\nfrom numpy import ndarray\nfrom ftfy import fix_encoding\n# data/parameter loading\nimport pandas as pd \nfrom math import sqrt\nfrom transformers import AutoConfig\nimport pickle\n# visualization\nfrom tqdm.notebook import trange, tqdm\nfrom accelerate.utils import set_seed\n# transfomers\nfrom transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup, DataCollatorWithPadding\nfrom transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions\nfrom transformers import AdamW\nfrom nltk.corpus import stopwords\nimport re\nfrom typing import Union, List, Dict\nimport gc\nfrom accelerate import Accelerator, notebook_launcher\nfrom accelerate.utils import set_seed\nfrom accelerate import DistributedDataParallelKwargs\n# filter out warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)","metadata":{"id":"xtofZSMi8uMe","execution":{"iopub.status.busy":"2023-03-31T04:41:51.900554Z","iopub.execute_input":"2023-03-31T04:41:51.901099Z","iopub.status.idle":"2023-03-31T04:41:58.462251Z","shell.execute_reply.started":"2023-03-31T04:41:51.901056Z","shell.execute_reply":"2023-03-31T04:41:58.461157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Some useful functions","metadata":{}},{"cell_type":"code","source":"# Utils\ndef save_parameter(save_object, save_file):\n    with open(save_file, 'wb') as f:\n        pickle.dump(save_object, f, protocol=pickle.HIGHEST_PROTOCOL)\n\ndef load_parameter(load_file):\n    with open(load_file, 'rb') as f:\n        output = pickle.load(f)\n    return output\n\ndef sim_matrix(a, b, eps=1e-8):\n    a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n    a_norm = a / torch.clamp(a_n, min=eps)\n    b_norm = b / torch.clamp(b_n, min=eps)\n    sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n    return sim_mt\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef compute_metrics(logits, labels):\n    topks = (1, 3, 5, 10)\n    scores = {k: 0 for k in topks}\n    sorted_logits = torch.argsort(torch.exp(logits), axis=1, descending=True)\n    num_cnt = {k: 0 for k in topks}\n    for k in topks:\n        batch_num_correct = 0\n        n_points = len(labels)\n        for idx in range(n_points):\n            if labels[idx] in sorted_logits[idx, 0:k]:\n                batch_num_correct += 1\n        scores[k] = batch_num_correct/n_points\n        num_cnt[k]+= batch_num_correct\n    return scores, num_cnt\n\ndef cosine_distance(x):\n    dist = 1-torch.mm(x, x.T)\n    dist = dist.triu(diagonal=1).mean()\n    return dist\n\ndef preprocess_keywords(keyword):\n    keyword=  keyword.split(\",\")\n    keyword = [ele.strip() for ele in keyword if len(ele.strip())>3]\n    keyword = \", \".join(keyword)\n    return keyword\n\nstop_words = set(stopwords.words('english'))","metadata":{"id":"zcZq_1j38vl-","execution":{"iopub.status.busy":"2023-03-31T04:41:58.467004Z","iopub.execute_input":"2023-03-31T04:41:58.470045Z","iopub.status.idle":"2023-03-31T04:41:58.494589Z","shell.execute_reply.started":"2023-03-31T04:41:58.470009Z","shell.execute_reply":"2023-03-31T04:41:58.493417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"markdown","source":"## Build set of paired samples","metadata":{}},{"cell_type":"code","source":"data_train = pd.read_csv(\"/kaggle/input/new-preprocessed-data/news_preprocessed_train (1).csv\")\ndata_test = pd.read_csv(\"/kaggle/input/new-preprocessed-data/news_preprocessed_test (1).csv\")\ndata_valid = pd.read_csv(\"/kaggle/input/new-preprocessed-data/news_preprocessed_valid (1).csv\")\ndata_aims = pd.read_csv(\"/kaggle/input/preprocessed-aims/preprocessed_aims (1) (1).csv\")\n\ndata_train = data_train.merge(data_aims[['itr', 'Aims', 'name']], on=\"itr\")\ndata_valid = data_valid.merge(data_aims[['itr', 'Aims', 'name']], on=\"itr\")\ndata_test = data_test.merge(data_aims[['itr', 'Aims', 'name']], on=\"itr\")\n\ndata_train.fillna(\"\", inplace=True)\ndata_valid.fillna(\"\", inplace=True)\ndata_test.fillna(\"\", inplace=True)\ndata_aims.fillna(\"\", inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T04:41:58.500707Z","iopub.execute_input":"2023-03-31T04:41:58.502830Z","iopub.status.idle":"2023-03-31T04:42:12.424471Z","shell.execute_reply.started":"2023-03-31T04:41:58.502795Z","shell.execute_reply":"2023-03-31T04:42:12.423482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"malteos/scincl\")\njournal = sorted(list(set(data_aims['name'].tolist())))\nlabel_dict = {journal: idx for idx, journal in enumerate(journal)}\ndata_train['Label'] = data_train['name'].map(label_dict)\ndata_valid['Label'] = data_valid['name'].map(label_dict)\ndata_test['Label'] = data_test['name'].map(label_dict)\nn_classes = len(journal)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-03-31T04:42:12.425835Z","iopub.execute_input":"2023-03-31T04:42:12.426686Z","iopub.status.idle":"2023-03-31T04:42:24.891610Z","shell.execute_reply.started":"2023-03-31T04:42:12.426648Z","shell.execute_reply":"2023-03-31T04:42:24.890660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aims = journal\nfor idx in range(len(aims)):\n    aims[idx] = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", aims[idx])\n    aims[idx] = aims[idx].replace(\",\", \"\").replace(\":\", \"\").replace(\".\", \"\").strip()\n    aims[idx] = word_tokenize(aims[idx])\n    aims[idx] = [w for w in aims[idx] if not w.lower().strip() in stop_words]\n    aims[idx] = \" \".join(aims[idx])\n    \naims_ids = tokenizer(aims, max_length=16, return_tensors='pt', padding=\"max_length\", truncation=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T04:42:24.893149Z","iopub.execute_input":"2023-03-31T04:42:24.893572Z","iopub.status.idle":"2023-03-31T04:42:24.952256Z","shell.execute_reply.started":"2023-03-31T04:42:24.893533Z","shell.execute_reply":"2023-03-31T04:42:24.951010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load saved pairs for training","metadata":{}},{"cell_type":"code","source":"class DualPaperDataset(torch.utils.data.Dataset):\n    def __init__(self, data, tokenizer, n_classes) -> None:\n        super().__init__()\n        self.data = data\n        self.tokenizer = tokenizer\n        self.n_classes = n_classes\n    \n    def __getitem__(self, index):\n        label = torch.tensor(self.data['Label'][index])\n        title = self.data['title'][index].strip() if isinstance(self.data['title'][index], str) else \"\"\n        title = f\"main idea : {title} . \"\n        abstract = self.data['abstract'][index].strip() if isinstance(self.data['abstract'][index], str) else \"\"\n        abstract = f\"concise summary : {abstract} . \"\n        keywords = self.data['keywords'][index].strip() if isinstance(self.data['keywords'][index], str) else \"\"\n        keywords = f\"important words : {keywords} .\"\n        context = title + abstract + keywords\n        context = self.tokenizer(context, max_length=320, truncation=True, padding=False)\n        context['label'] = label\n        return context\n    \n    def __len__(self):\n        return len(self.data)","metadata":{"executionInfo":{"elapsed":217610,"status":"ok","timestamp":1648819636617,"user":{"displayName":"Tram Doan","userId":"12218878138740774355"},"user_tz":-420},"id":"5NciRrcT82Xs","outputId":"7a6c94ed-48f4-4570-f9d9-4b9aea0a233a","execution":{"iopub.status.busy":"2023-03-31T04:42:24.954092Z","iopub.execute_input":"2023-03-31T04:42:24.954533Z","iopub.status.idle":"2023-03-31T04:42:24.966810Z","shell.execute_reply.started":"2023-03-31T04:42:24.954498Z","shell.execute_reply":"2023-03-31T04:42:24.965399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model definition","metadata":{}},{"cell_type":"markdown","source":"# Model for contrastive leanring training","metadata":{}},{"cell_type":"code","source":"class StackAttentionLayer(nn.Module):\n    def __init__(self, embeds_dim, n_classes):\n        super().__init__()\n        self.linear_query = nn.Linear(embeds_dim, embeds_dim, bias=False)\n        self.linear_key = nn.Linear(embeds_dim, embeds_dim, bias=False)\n        self.attn_linear = nn.Linear(n_classes, 1, bias=True)\n        self.scale = sqrt(embeds_dim)\n        self.layer_norm1 = nn.LayerNorm(embeds_dim, eps=1e-12)\n        self.mlp = nn.Sequential(\n            nn.Linear(embeds_dim, embeds_dim),\n            nn.ReLU(),\n            nn.Linear(embeds_dim, embeds_dim),\n            nn.ReLU(),\n            nn.Linear(embeds_dim, embeds_dim)\n        )\n        self.layer_norm2 = nn.LayerNorm(embeds_dim, eps=1e-12)\n\n    def forward(self, input_feats, label_feats, attn_mask=None):\n        '''\n        input_feats size: BxSxD\n        label_feats size: CxD\n        '''\n        residual = input_feats[:, 0, :]\n        input_feats = self.linear_query(input_feats)\n        label_feats = self.linear_key(label_feats)\n        dot_product = torch.div(torch.matmul(input_feats, label_feats.T), self.scale)\n        # dot product: BxM\n        attn = self.attn_linear(dot_product).squeeze()\n        attn = attn.masked_fill_(attn_mask.eq(0), value=float('-inf'))\n        attn = F.softmax(attn, dim=-1)\n        out = torch.einsum('bc, bcd->bd', attn, input_feats)\n        residual = self.layer_norm1(out+residual)\n        out = self.layer_norm2(self.mlp(out) + residual)\n        return out\n\nclass PaperModel(nn.Module):\n    def __init__(self, hidden_size, model_name_or_path, num_classes) -> None:\n        super(PaperModel, self).__init__()\n        if isinstance(model_name_or_path, str):\n            self.encoder = AutoModel.from_pretrained(model_name_or_path)\n        else:\n            self.encoder = model_name_or_path\n        self.n_classes = num_classes\n        self.temperature =  nn.Parameter(torch.ones([]) * 0.07)\n        self.attn = StackAttentionLayer(hidden_size, num_classes)\n        for param in self.encoder.parameters():\n            param.requires_grad_(True)\n        \n    def get_label_feats(self, aims_ids):\n        label_feats = self.encoder(**aims_ids).last_hidden_state[:, 0, :]\n        return label_feats\n        \n    def forward(self, inputs, aims_ids):\n        with torch.no_grad():\n            self.temperature.clamp_(0.01,0.5)\n        hiddens = self.encoder(**inputs).last_hidden_state\n        label_feats = self.get_label_feats(aims_ids)\n#         hiddens = hiddens[:, 0, :]\n        hiddens = self.attn(hiddens, label_feats, inputs['attention_mask'])\n        hiddens = F.normalize(hiddens, dim=-1)\n        label_feats = F.normalize(label_feats, dim=-1)\n        logits = torch.einsum(\"bd, cd->bc\", hiddens, label_feats)/self.temperature\n        outputs = {\n            \"logits\": logits, \"cls_feats\": hiddens, \"label_feats\": label_feats\n        }\n        return outputs","metadata":{"id":"7HG4b4GT8oCz","execution":{"iopub.status.busy":"2023-03-31T04:42:24.968612Z","iopub.execute_input":"2023-03-31T04:42:24.968976Z","iopub.status.idle":"2023-03-31T04:42:24.985921Z","shell.execute_reply.started":"2023-03-31T04:42:24.968940Z","shell.execute_reply":"2023-03-31T04:42:24.984927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pooler","metadata":{}},{"cell_type":"markdown","source":"## Contrastive Loss","metadata":{}},{"cell_type":"code","source":"def mean_pooling(model_output, attention_mask):\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(model_output.size()).float()\n    return torch.sum(model_output * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\ndef sum_pooling(model_output, attention_mask):\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(model_output.size()).float()\n    return torch.sum(model_output * input_mask_expanded, 1)\n\nclass CELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.xent_loss = nn.CrossEntropyLoss()\n        \n    def forward(self, outputs, targets):\n        return self.xent_loss(outputs['logits'], targets)\n\nclass SupConLoss(nn.Module):\n\n    def __init__(self, alpha, temp):\n        super().__init__()\n        self.xent_loss = nn.CrossEntropyLoss(label_smoothing=0.1)\n        self.alpha = alpha\n        self.temp = temp\n\n    def nt_xent_loss(self, anchor, target, labels):\n        with torch.no_grad():\n            labels = labels.unsqueeze(-1)\n            mask = torch.eq(labels, labels.transpose(0, 1))\n            # delete diag elem\n            mask = mask ^ torch.diag_embed(torch.diag(mask))\n        # compute logits\n        anchor_dot_target = torch.einsum('bd,cd->bc', anchor, target) / self.temp\n        # delete diag elem\n        anchor_dot_target = anchor_dot_target - torch.diag_embed(torch.diag(anchor_dot_target))\n        # for numerical stability\n        logits_max, _ = torch.max(anchor_dot_target, dim=1, keepdim=True)\n        logits = anchor_dot_target - logits_max.detach()\n        \n        exp_logits = torch.exp(logits)\n        # mask out positives\n        logits = logits * mask\n        log_prob = logits - torch.log(exp_logits.sum(dim=1, keepdim=True) + 1e-12)\n        # in case that mask.sum(1) is zero\n        mask_sum = mask.sum(dim=1)\n        mask_sum = torch.where(mask_sum == 0, torch.ones_like(mask_sum), mask_sum)\n        # compute log-likelihood\n        pos_logits = (mask * log_prob).sum(dim=1) / mask_sum.detach()\n        loss = -1 * pos_logits.mean()\n        return loss\n\n    def forward(self, outputs, targets):\n        normed_cls_feats = F.normalize(outputs['cls_feats'], dim=-1)\n        ce_loss = (1 - self.alpha) * self.xent_loss(outputs['logits'], targets)\n        cl_loss = self.alpha * self.nt_xent_loss(normed_cls_feats, normed_cls_feats, targets)\n        return ce_loss + cl_loss\n\nclass DualLoss(SupConLoss):\n    def __init__(self, alpha=0.0, temp=0.1):\n        super().__init__(alpha, temp)\n\n    def forward(self, outputs, targets):\n        cls_feats = outputs['cls_feats']\n        label_feats = outputs['label_feats']\n        ce_loss = self.xent_loss(outputs['logits'], targets)\n        return ce_loss\n\nclass LabelRegLoss(nn.Module):\n    def __init__(self, threshold=0.5, is_normalize=True):\n        super().__init__()\n        self.is_normalize = is_normalize\n        self.threshold = threshold\n    \n    def forward(self, x):\n        if self.is_normalize == False:\n            x = F.normalize(x, dim=-1)\n        sim_matrix = torch.mm(x, x.T)\n        mask = torch.diag_embed(torch.ones_like(sim_matrix.diagonal(dim1=0, dim2=1)))\n        sim_matrix = sim_matrix.masked_fill(mask == 1, float('-inf'))\n        sim_matrix, _ = torch.max(sim_matrix, dim=-1)\n        sim_matrix = F.relu(sim_matrix-self.threshold)\n        loss = sim_matrix.mean()\n        return loss","metadata":{"id":"ajsMcTpN9YpD","execution":{"iopub.status.busy":"2023-03-31T04:42:24.987413Z","iopub.execute_input":"2023-03-31T04:42:24.987958Z","iopub.status.idle":"2023-03-31T04:42:25.007677Z","shell.execute_reply.started":"2023-03-31T04:42:24.987923Z","shell.execute_reply":"2023-03-31T04:42:25.006730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model declaration","metadata":{}},{"cell_type":"code","source":"model = AutoModel.from_pretrained(\"malteos/scincl\")\n\n# layers_to_keep = [0, 2, 4, 6, 9, 11]\n# new_layers = torch.nn.ModuleList([layer_module for i, layer_module in enumerate(model.encoder.layer) if i in layers_to_keep])\n# model.encoder.layer = new_layers\n# model.config.num_hidden_layers = len(layers_to_keep)\n# state = torch.load(\"/kaggle/input/distilscincl-state-dict/Epoch_1_steps8801_student_embedding.pth\")\n# model.load_state_dict(state['model_state_dict'])\nmodel_args = {\n    \"hidden_size\": 768,\n    'model_name_or_path': model,\n    \"num_classes\": n_classes\n}","metadata":{"executionInfo":{"elapsed":19330,"status":"ok","timestamp":1648819656682,"user":{"displayName":"Tram Doan","userId":"12218878138740774355"},"user_tz":-420},"id":"laqFK23B9p3u","outputId":"a2691f8c-a168-4a86-acde-331784dfeb0c","execution":{"iopub.status.busy":"2023-03-31T04:42:25.011974Z","iopub.execute_input":"2023-03-31T04:42:25.012405Z","iopub.status.idle":"2023-03-31T04:42:43.263652Z","shell.execute_reply.started":"2023-03-31T04:42:25.012380Z","shell.execute_reply":"2023-03-31T04:42:43.262654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Optimizer and configuration","metadata":{}},{"cell_type":"code","source":"def training_loop(aims_ids, mixed_precision=\"fp16\", seed=42, batch_size=32, state=None):\n    set_seed(seed)\n    torch.cuda.empty_cache()\n    gc.collect()\n    history = {\"cl_loss\": [], \"uniform_loss\": [], \"ce_loss\": [], \"accuracy\": []}\n    accelerator = Accelerator(mixed_precision=mixed_precision)\n    data_collator = DataCollatorWithPadding(tokenizer)\n    model = PaperModel(**model_args)\n    dataset = DualPaperDataset(data_train, tokenizer, n_classes)\n    data_loader = torch.utils.data.DataLoader(dataset, collate_fn=data_collator, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n    min_loss = np.inf\n    valid_dataset = DualPaperDataset(data_valid, tokenizer, n_classes)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, collate_fn=data_collator, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n    max_epochs = 3\n    print(\"Model summary:\\n\")\n    print(\">> Total params: \", count_parameters(model))\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    \n    optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, weight_decay=1e-2, correct_bias=True)\n    num_training_steps = len(data_loader)*max_epochs\n    num_warmup_steps = int(num_training_steps*0.1)\n    lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n    criterion = DualLoss()\n    saved_epochs = -1\n    uniform_criterion = LabelRegLoss()\n    if state != None:\n        model.load_state_dict(state['model_state_dict'])\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        lr_scheduler.load_state_dict(state['scheduler_state_dict'])\n        saved_epochs = state['epoch']\n    aims_ids = {k:v.to(accelerator.device) for k, v in aims_ids.items()}\n    topks = (1,3,5,10)\n    print(f\"Saved epochs: {saved_epochs+1}\")\n    model, uniform_criterion, optimizer, data_loader, valid_loader, lr_scheduler = accelerator.prepare(\n    model, uniform_criterion, optimizer, data_loader, valid_loader, lr_scheduler)\n    for epoch in range(saved_epochs+1, max_epochs):\n        loop = tqdm(data_loader, leave=True, disable=not accelerator.is_local_main_process)\n        train_loss = 0.0\n        train_score = {k:0 for k in (1,3,5,10)}\n        for idx, batch in enumerate(loop):\n            inputs = {k:v.squeeze() for k, v in batch.items() if k != 'labels'}\n            labels = batch['labels']\n            optimizer.zero_grad()\n            outputs = model(inputs, aims_ids)\n            label_feats = outputs['label_feats']\n            ce_loss =  criterion(outputs, labels)\n            uniform_loss = uniform_criterion(label_feats)\n            loss = ce_loss + 0.1*uniform_loss\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            history['ce_loss'].append(ce_loss.item())\n            history['uniform_loss'].append(uniform_loss.item())\n            train_loss += loss.item()\n            \n            with torch.no_grad():\n                cosine_dist = cosine_distance(label_feats.detach().clone())\n                score, num_cnt = compute_metrics(outputs['logits'], labels)\n                history['accuracy'].append(score[1])\n                for k in topks:\n                    train_score[k] += num_cnt[k]\n            if idx % 300 == 0:\n                accelerator.print(f\"Loss: {loss.item()} || Temp: {model.temperature.item()}|| Regularize loss: {uniform_loss.item():.5f} || Uniformity: {cosine_dist} || Top 1 acc: {score[1]} || Top 3 acc: {score[3]} || Top 5 acc: {score[5]} || Top 10 acc: {score[10]}\")\n            loop.set_description('Epoch: {} - lr: {}'.format(epoch+1, optimizer.param_groups[0]['lr']))\n            loop.set_postfix(loss=round(loss.item(), 3), temp = model.temperature.item(), top01=score[1], top03=score[3], top05=score[5], top10=score[10])\n        train_loss = train_loss / (len(dataset))\n        for k in topks:\n            accelerator.print(f\"Train top {k} acc: {train_score[k]/(len(dataset))}\", end=\" || \")\n        accelerator.print(\"\")\n        valid_loss = 0.0\n        valid_loop = tqdm(valid_loader, leave=True, disable=not accelerator.is_local_main_process)\n        valid_score = {k:0 for k in(1,3,5,10)}\n        for batch in valid_loop:\n            with torch.no_grad():\n                inputs = {k:v.squeeze() for k, v in batch.items() if k != 'labels'}\n                labels = batch['labels']\n                inputs = {k:v.squeeze() for k, v in inputs.items()}\n                outputs = model(inputs, aims_ids)\n                loss = criterion(outputs, labels)\n                valid_loss += loss.item()\n                score, num_cnt = compute_metrics(outputs['logits'], labels)\n                for k in topks:\n                    valid_score[k] += num_cnt[k]\n            valid_loop.set_description('Epoch: {} - lr: {} '.format(epoch+1, optimizer.param_groups[0]['lr']))\n            valid_loop.set_postfix(loss=loss.item(), top01=score[1], top03=score[3], top05=score[5], top10=score[10])\n        valid_loss /= len(valid_loader)\n        for k in topks:\n            accelerator.print(f\"Valid top {k} acc: {valid_score[k]/len(valid_dataset)}\", end=\" || \") \n        accelerator.print(\"\")\n        print(f'Validation Loss ({min_loss:.6f}--->{valid_loss:.6f})')\n        min_loss = valid_loss\n        accelerator.save({\n            \"history\": history,\n            \"model_state_dict\": model.state_dict(),\n            \"optimizer_state_dict\": optimizer.state_dict(),\n            \"scheduler_state_dict\": lr_scheduler.state_dict(),\n            \"epoch\": epoch\n                }, \"./Epoch:{:0>2} SupCL-SciNCL.pth\".format(epoch+1))","metadata":{"id":"_7WuBq3j-mHb","execution":{"iopub.status.busy":"2023-03-31T04:42:43.265037Z","iopub.execute_input":"2023-03-31T04:42:43.265390Z","iopub.status.idle":"2023-03-31T04:42:43.299848Z","shell.execute_reply.started":"2023-03-31T04:42:43.265355Z","shell.execute_reply":"2023-03-31T04:42:43.297681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = (aims_ids, \"fp16\", 42, 32)\n\ntraining_loop(*args)","metadata":{"execution":{"iopub.status.busy":"2023-03-31T04:42:43.301264Z","iopub.execute_input":"2023-03-31T04:42:43.301643Z","iopub.status.idle":"2023-03-31T04:48:29.317097Z","shell.execute_reply.started":"2023-03-31T04:42:43.301608Z","shell.execute_reply":"2023-03-31T04:48:29.315362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# topks = (1, 3, 5, 10) \n# train_score = {k:0 for k in (1,3,5,10)}\n# valid_score = {k:0 for k in (1,3,5,10)}\n# test_score = {k:0 for k in (1,3,5,10)}\n# model = PaperModel(**model_args)\n# state = torch.load(\"/kaggle/input/scincl-base-best-checkpoint/Epoch_03 SupCL-DistilRoberta (4).pth\")\n# model.load_state_dict(state['model_state_dict'])\n# model.eval()\n# train_loss = 0.0\n# valid_loss = 0.0\n# test_loss = 0.0\n# criterion_ce = nn.CrossEntropyLoss()\n# accelerator = Accelerator()\n# dataset = DualPaperDataset(data_train, tokenizer, n_classes)\n# data_collator = DataCollatorWithPadding(tokenizer)\n# valid_dataset = DualPaperDataset(data_valid, tokenizer, n_classes)\n# valid_loader = torch.utils.data.DataLoader(valid_dataset, collate_fn=data_collator, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n# test_dataset = DualPaperDataset(data_test, tokenizer, n_classes)\n# test_loader = torch.utils.data.DataLoader(test_dataset, collate_fn=data_collator, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n# model, valid_loader, test_loader = accelerator.prepare(\n# model, valid_loader, test_loader)\n# aims_ids = {k: v.to(accelerator.device) for k, v in aims_ids.items()}\n\n# valid_loop = tqdm(valid_loader, leave=True)\n# for batch in valid_loop:\n#     with torch.no_grad():\n#         inputs = {k:v.squeeze() for k, v in batch.items() if k != 'labels'}\n#         labels = batch['labels']\n#         logits = model(inputs, aims_ids)\n#         logits = logits['logits']\n#         loss = criterion_ce(logits, labels)\n#         test_loss += loss.item()\n#         score, num_cnt = compute_metrics(logits, labels)\n#         valid_loop.set_postfix(Top_01=score[1], Top_03=score[3], Top_05=score[5], Top_10=score[10])\n#         for k in (1, 3, 5, 10):\n#             valid_score[k] += num_cnt[k]\n\n# test_loop = tqdm(test_loader, leave=True)\n# for batch in test_loop:\n#     with torch.no_grad():\n#         inputs = {k:v.squeeze() for k, v in batch.items() if k != 'labels'}\n#         labels = batch['labels']\n#         logits = model(inputs, aims_ids)\n#         logits = logits['logits']\n#         loss = criterion_ce(logits, labels)\n#         test_loss += loss.item()\n#         score, num_cnt = compute_metrics(logits, labels)\n#         test_loop.set_postfix(Top_01=score[1], Top_03=score[3], Top_05=score[5], Top_10=score[10])\n#         for k in (1, 3, 5, 10):\n#             test_score[k] += num_cnt[k]\n\n# test_loss /= len(test_loader)\n\n# print(f\"Test loss: {test_loss:.3f}\")\n# print(\"\")\n\n# for k in topks:\n#     print(f\"Test top{k} acc: {test_score[k]/len(test_dataset)}\")","metadata":{"execution":{"iopub.status.busy":"2023-03-31T04:48:29.318651Z","iopub.status.idle":"2023-03-31T04:48:29.319369Z","shell.execute_reply.started":"2023-03-31T04:48:29.319112Z","shell.execute_reply":"2023-03-31T04:48:29.319137Z"},"trusted":true},"execution_count":null,"outputs":[]}]}